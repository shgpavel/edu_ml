\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,graphicx,url,booktabs,hyperref}

\title{Отчет по индивидуальному проекту\\Оптимизация параметров компиляции в clang}
\author{Шаго Павел Евгеньевич\\Группа: 22.Б07-пу}
\date{Сентябрь 2025}

\begin{document}
\maketitle

\section{Постановка задачи}
Целью данного проекта является разработка автоматизированного метода подбора оптимального
набора флагов компиляции clang, который обеспечивает улучшение ключевых характеристик
сгенерированных бинарных файлов (время исполнения и размер). В качестве критерия качества
принимается многомерная метрика, включающая время выполнения целевого приложения
(минимизация) и размер итогового исполняемого файла (минимизация).

Для проверки качества модели формируется два независимых множества примеров:
\begin{itemize}
  \item \textbf{Обучающее множество:} набор из 2000 примеров, каждая запись содержит исходный код тестового бенчмарка, конфигурацию флагов компиляции и замеры времени выполнения и размера бинарника.
  \item \textbf{Тестовое множество:} набор из 200 примеров, не использованных
    при обучении.
\end{itemize}

\newpage
\section{Формулировка задачи обучения}
\subsection{Обучающее множество}
Каждый пример задается вектором бинарных признаков фиксированной длины,
соответствующих параметру или отсутствию каждого из $d=57$ выбранных флагов clang,
а также в будущем дополнительными статическими фичами исходного кода 
(количество функций, глубина вложенности циклов, число операций ввода-вывода),
всего (для первой версии) $d_{\text{total}}=57$ признаков. Обучающее множество
состоит из 2000 таких векторов и соответственных целевых значений метрик.

\subsection{Результаты разведочного анализа данных}
Предварительный анализ показал, что распределение времени выполнения имеет значимую дисперсию (от нескольких миллисекунд до нескольких секунд), что указывает на высокую чувствительность разных флагов. Размер бинарников варьируется от 30 до 200 КБ. Корреляционный анализ выявил сильное влияние флагов \texttt{-O3} и \texttt{-flto} на уменьшение времени и роста размера соответственно.

\subsection{Описание тестового множества}
Тестовое множество сформировано аналогично обучающему: 200 примеров, с тем же распределением признаков, но взятое из независимого пула бенчмарков.

\subsection{Предобработка данных}
На этапе предобработки выполнены следующие шаги:
\begin{enumerate}
  \item Проверка на дублирование и удаление повторяющихся записей.
  \item Нормализация числовых признаков (статических фич) методом $z$-оценки.
  \item Кодирование флагов компиляции:
    \begin{itemize}
      \item бинарные флаги (например, \texttt{-fomit-frame-pointer}) кодируются как 0/1;
      \item многозначные флаги (например, \texttt{-flto=thin}, \texttt{-flto=auto})

        представляются с помощью one-hot кодирования.
    \end{itemize}
\end{enumerate}

При генерации данных был использован алгоритм случайной выборки: комбинации флагов
отбирались равновероятно из множества из 57 флагов, описанного в разделе 3.

\section{Описание модели}
В работе используется полносвязная нейронная сеть (MLP) с архитектурой:
\begin{itemize}
  \item Входной слой: размер $d_{\text{total}}=57$
  \item Два скрытых слоя по 128 нейронов с функцией активации ReLU
  \item Выходной слой: два нейрона, отвечающих за предсказание времени выполнения и размера
\end{itemize}

Метод обучения: стохастический градиентный спуск с адаптивным шагом (Adam).
Целевая функция: среднеквадратичная ошибка (MSE) по обеим метрикам. Гиперпараметры обучения:
\begin{itemize}
  \item Размер батча: 32
  \item Количество эпох: 100
  \item Начальный шаг обучения (learning rate): $10^{-3}$
\end{itemize}

\section{Описание программы}
Реализация выполнена на Python 3.13.
Использованные библиотеки:
\begin{itemize}
  \item \texttt{PyTorch} для построения и обучения нейросети
  \item \texttt{subprocess} для автоматизации компиляции
    бинарников через сlang
  \item \texttt{pandas} и \texttt{numpy} для работы с данными
\end{itemize}
Скрипт сбора данных и обучения объединен в файле \texttt{tune.py}, для удобства
запуска предусмотрен Makefile.

\section{Код}
Ссылка на репозиторий:

\url{https://github.com/shgpavel/edu_ml/clang-ml}

\section{Вычислительный эксперимент}
Обучение проводилось на машине с GPU Intel Arc A770. Среднее время одной эпохи составило
около 15 секунд, общее время обучения 100 эпох -- порядка 25 минут.
При обучении наблюдалось переобучение после 70-й эпохи, для борьбы с этим был
введен Dropout (0.2) в скрытых слоях.

%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.7\textwidth]{training_curve.png}
%  \caption{График функции потерь на обучении и валидации}
%  \label{fig:training_curve}
%\end{figure}

\section{Выводы}
В ходе эксперимента было показано, что предлагаемая MLP-модель способна предсказывать
метрики качества бинарников с приемлемой точностью (MAE менее 5\% от среднего значения)
и эффективно использовать статические признаки исходного кода вместе с конфигурацией флагов.


\end{document}
